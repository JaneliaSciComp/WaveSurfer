import os
import posixpath
import math
import numpy
import h5py

def loadDataFile(filename, formatString='double') :
    # function dataFileAsDict = loadDataFile(filename,formatString)
    # Loads Wavesurfer data file.  The returned data is a structure array
    # with one element per sweep in the data file.

    # Deal with optional args
    #if "formatString" in locals() or len(formatString)==0 :
    #    formatString = "double"
    
    # Check that file exists
    if not os.path.isfile(filename) :
        raise FileNotFoundError("The file {:%s} does not exist.".format(filename))
    
    # Check that file has proper extension
    (dontcare, ext) = os.path.splitext(filename)
    if ext!=".h5" :
        raise RuntimeError("File must be a WaveSurfer-generated HDF5 (.h5) file.")

    # Extract dataset at each group level, recursively.    
    file = h5py.File(filename)
    dataFileAsDict = crawl_h5_group(file)  # an h5py File is also a Group
    
    # Correct the samples rates for files that were generated by versions
    # of WS which didn't coerce the sampling rate to an allowed rate.
    header = dataFileAsDict["header"]
    if "VersionString" in header :
        versionString = header["VersionString"] ;  # this is a scalar numpy array with a weird datatype
        version = float(versionString.tostring().decode('utf-8')) ;
    else:
        # If no VersionsString field, the file is from an old old version
        version = 0 ;
    if version < 0.9125 :  # version 0.912 has the problem, version 0.913 does not
        # Fix the acquisition sample rate, if needed
        nominalAcquisitionSampleRate = float(header["Acquisition"]["SampleRate"])
        nominalNTimebaseTicksPerSample = 100.0e6/nominalAcquisitionSampleRate 
        if nominalNTimebaseTicksPerSample != round(nominalNTimebaseTicksPerSample) :  # should use the python round, not numpy round
            actualAcquisitionSampleRate = 100.0e6/math.floor(nominalNTimebaseTicksPerSample) ;  # sic: the boards floor() for acq, but round() for stim
            header["Acquisition"]["SampleRate"] = numpy.array(actualAcquisitionSampleRate) 
            dataFileAsDict["header"] = header
        # Fix the stimulation sample rate, if needed
        nominalStimulationSampleRate = float(header["Stimulation"]["SampleRate"])
        nominalNTimebaseTicksPerSample = 100.0e6/nominalStimulationSampleRate ;
        if nominalNTimebaseTicksPerSample != round(nominalNTimebaseTicksPerSample) :
            actualStimulationSampleRate = 100.0e6/round(nominalNTimebaseTicksPerSample) ;  # sic: the boards floor() for acq, but round() for stim
            header["Stimulation"]["SampleRate"] = numpy.array(actualStimulationSampleRate)
            dataFileAsDict["header"] = header
    
    # If needed, use the analog scaling coefficients and scales to convert the
    # analog scans from counts to experimental units.
    if "NAIChannels" in header :
        nAIChannels = header["NAIChannels"]
    else:
        allAnalogChannelScales = header["Acquisition"]["AnalogChannelScales"] 
        nAIChannels = allAnalogChannelScales.size  # element count
    if formatString.lower()=="raw" or nAIChannels==0 :
        pass
    else :
        try :
            if "AIChannelScales" in header :
                # Newer files have this field, and lack header.Acquisition.AnalogChannelScales
                allAnalogChannelScales = header["AIChannelScales"]
            else :
                # Fallback for older files
                allAnalogChannelScales = header["Acquisition"]["AnalogChannelScales"] 
        except KeyError :
            raise KeyError("Unable to read channel scale information from file.")
        try :
            if "IsAIChannelActive" in header :
                # Newer files have this field, and lack header.Acquisition.AnalogChannelScales
                isActive = header["IsAIChannelActive"].astype(bool)
            else :
                # Fallback for older files
                isActive = header["Acquisition"]["IsAnalogChannelActive"].astype(bool)
        except KeyError :
            raise KeyError("Unable to read active/inactive channel information from file.")
        analogChannelScales = allAnalogChannelScales[isActive]
        
        # read the scaling coefficients
        try :
            if "AIScalingCoefficients" in header :
                analogScalingCoefficients = header["AIScalingCoefficients"]
            else :
                analogScalingCoefficients = header["Acquisition"]["AnalogScalingCoefficients"]
        except KeyError :
            raise KeyError("Unable to read channel scaling coefficients from file.")
        
        #inverseAnalogChannelScales=1./analogChannelScales  # if some channel scales are zero, this will lead to nans and/or infs
        doesUserWantSingle = (formatString.lower()=="single") 
        #doesUserWantDouble = ~doesUserWantSingle 
        #field_names = dataFileAsDict.keys()
        for field_name in dataFileAsDict :
            #field_names = field_namess{i}
            if len(field_name)>=5 and (field_name[0:5]=="sweep" or field_name[0:5]=="trial") :  
                # We check for "trial" for backward-compatibility with
                # data files produced by older versions of WS.
                analogDataAsCounts = dataFileAsDict[field_name]["analogScans"]
                if doesUserWantSingle :
                    scaledAnalogData = scaledSingleAnalogDataFromRaw(analogDataAsCounts, analogChannelScales, analogScalingCoefficients) 
                else :
                    scaledAnalogData = scaledDoubleAnalogDataFromRaw(analogDataAsCounts, analogChannelScales, analogScalingCoefficients) 
                dataFileAsDict[field_name]["analogScans"] = scaledAnalogData 
    
    return dataFileAsDict



def crawl_h5_group(group) :
    result = dict()
    
    item_names = list(group.keys())

    for item_name in item_names :
        item = group[item_name]
        if isinstance(item, h5py.Group) :
            field_name = field_name_from_hdf_name(item_name)
            result[field_name] = crawl_h5_group(item)
        elif isinstance(item, h5py.Dataset) :
            field_name = field_name_from_hdf_name(item_name)
            result[field_name] = item[()]
        else :
            pass      

    return result



#def get_group_info(hdf5_group) :
#    # fid is not a FID, it's a h5py.File
#    item_names = list(hdf5_group.keys())
#
#    subgroup_names = []
#    dataset_names = []
#    
#    for item_name in item_names :
#        item = hdf5_group[item_name]
#        if isinstance(item, h5py.Group) :
#            subgroup_names = append(subgroup_names, item_name)
#        elif isinstance(item, h5py.Dataset) :
#            dataset_names = append(dataset_names, item_name)
#        else :
#            pass      
#
#    return (dataset_names, subgroup_names)



def field_name_from_hdf_name(hdf_name) :
    # Convert the name of an HDF dataset/group to something that is a legal
    # Matlab struct field name.  We do this even in Python, just to be 
    # consistent.
    try :
        hdf_name_as_double = float(hdf_name)
        isHDFNameNumeric = True
    except ValueError :
        isHDFNameNumeric = False    

    if isHDFNameNumeric :
        # the group/dataset name seems to be a number.  If it's an integer, we can deal, so check that.
        if hdf_name_as_double == round(hdf_name_as_double) :
            # If get here, group name is an integer, so we prepend with an "n" to get a valid field name    
            field_name = "n{:%s}".format(hdf_name)
        else :
            # Not an integer.  Give up.
            raise RuntimeError("Unable to convert group/dataset name {:%s} to a valid field name.".format(hdf_name))
    else :
        # This is actually a good thing, b/c it means the groupName is not
        # simply a number, which would be an illegal field name
        field_name = hdf_name
        
    return field_name



def local_hdf_name_from_path(rawPath) :
    # Given the path to an HDF group/dataset, get the name of the item within its parent group.
    # This is sometimes called the "leaf" name.
    # If the input ends in a "/", we pretend like the "/" isn't there.
    if len(rawPath)==0 :
        localName = ""
    else :
        if rawPath[-1]=="/" :
            path = rawPath[0:-1]
        else :
            path = rawPath
        (dontcare, localName) = posixpath.split(path)
    return localName



def scaledDoubleAnalogDataFromRaw(dataAsADCCounts, channelScales, scalingCoefficients) :    
    # Function to convert raw ADC data as int16s to doubles, taking to the
    # per-channel scaling factors into account.
    #
    #   dataAsADCCounts: nChannels x nScans int16 array
    #   channelScales: double vector of length nChannels, each element having
    #                   (implicit) units of V/(native unit), where each
    #                   channel has its own native unit.
    #   scalingCoefficients: nChannels x nCoefficients  double array,
    #                        contains scaling coefficients for converting
    #                        ADC counts to volts at the ADC input.
    #
    #   scaledData: nScans x nChannels double array containing the scaled
    #               data, each channel with it's own native unit.
    
    inverseChannelScales = 1.0/channelScales  # if some channel scales are zero, this will lead to nans and/or infs
    nChannels = channelScales.size
#    if nChannels==1 :
#        # support single-channel case (seems like there must be a more elegant way to do this)
#        scaledData = inverseChannelScales * numpy.polyval(numpy.flipud(scalingCoefficients), dataAsADCCounts)
#    else :
    scaledData = numpy.empty(dataAsADCCounts.shape)
    for i in range(0, nChannels) :
        scaledData[i, :] = inverseChannelScales[i] * numpy.polyval(numpy.flipud(scalingCoefficients[i, :]), dataAsADCCounts[i, :])
    return scaledData



def scaledSingleAnalogDataFromRaw(dataAsADCCounts, channelScales, scalingCoefficients) :    
    scaledData = scaledDoubleAnalogDataFromRaw(dataAsADCCounts, channelScales, scalingCoefficients)
    return scaledData.astype('single')

#d = loadDataFile('./test2.h5')


    
